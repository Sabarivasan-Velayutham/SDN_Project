# -*- coding: utf-8 -*-
"""Copy of popularity_with_crct_data_trying_out_reward_thingyy.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1G50oLaBBNp1NnlAHD2HsnBDNWLRSm68V
"""

import csv
import pandas as pd
import ast

# Define a function to read CSV files and extract values from the second column
def read_csv(file_name):
    values = []
    with open(file_name, 'r', newline='', encoding='utf-8') as file:
        reader = csv.reader(file)
        next(reader)  # Skip the header row if it exists
        for row in reader:
          if row[1] != "":
            values.append(int(float(row[1])))  # Append the value from the second column
    return values

# List of file names
file_names = ['Molotov.csv', 'Orange.csv', 'Twitter.csv', 'Web Games.csv', 'YouTube.csv', 'EA_Games.csv', 'Linkedin.csv']

# Dictionary to store arrays for each file
data =  {}

# Read each file and store values in the dictionary
for file_name in file_names:
    data[file_name[:-4]] = read_csv(file_name)

# Example usage: Accessing the array of values for each file
for file_name, values in data.items():
    print("Values for", file_name, ":", values)

# Dictionary to store arrays for each position
position_data = {}

# Determine the maximum length of arrays
max_length = max(len(values) for values in data.values())

# Iterate through each position
for i in range(max_length):
    position_array = []
    for file_name, values in data.items():
        if i < len(values):
            position_array.append(values[i])
        else:
            position_array.append(None)  # If the array is shorter, add None as a placeholder
    position_data["Position " + str(i + 1)] = position_array

# Example usage: Accessing the arrays for each position
for position, values in position_data.items():
    print(position + ":", values)

# Function to chunk data into smaller lists of size n
def chunk_list(lst, n):
    return [lst[i:i+n] for i in range(0, len(lst), n)]

# Rearrange data into the desired format
new_data = []
for i in range(max_length):
    new_row = []
    for file_name, values in data.items():
        if i < len(values):
            # print(values[i])
            new_row.append(values[i])
        else:
            new_row.append(None)  # If the array is shorter, add None as a placeholder
        # print(new_row)
    new_data.append(new_row)


# Chunk the new_data into smaller lists of size 960
chunked_data = chunk_list(new_data, 960)

# Prepare column names
columns = ['T' + str(i + 1) for i in range(len(chunked_data[0]))]

# Prepare row names
rows = ['L' + str(i + 1) for i in range(len(chunked_data))]

# Write the chunked_data to a new CSV file
with open('predicted_data.csv', 'w', newline='', encoding='utf-8') as file:
    writer = csv.writer(file)

    # Write column names
    writer.writerow([''] + columns)

    # Write row names and data
    for row_name, row_data in zip(rows, chunked_data):
        writer.writerow([row_name] + row_data)

csv_file_path = 'predicted_data.csv'

with open(csv_file_path, 'r', newline='') as file:
    csv_reader = csv.reader(file)
    next(csv_reader)
    numeric_data = [[eval(value) for value in row[1:]] for row in csv_reader]


days = 10
timeslots = 96
locations = 5
services = 7

results = []

for location in range(locations):
    location_results = []

    for timeslot in range(timeslots):
        # Get the arrays from all 4 days for the current time slot
        arrays_per_day = [numeric_data[location][day * timeslots + timeslot] for day in range(days)]

        total = [sum(value for value in service_presence) for service_presence in zip(*arrays_per_day)]

        location_results.append(total)

    results.append(location_results)

for i in range(len(results)):
  print("Location  "+ str(i + 1)+" :  " + str(results[i]))



print("\n \npopular services \n \n")
popular = []


for location_results in results:
    popular_for_one = []

    for total in location_results:
        # Get the indexes in descending order based on sum
        sorted_indexes = sorted(range(len(total)), key=lambda i: total[i], reverse=True)
        popular_for_one.append(sorted_indexes)

    popular.append(popular_for_one)


for i in range(len(popular)):
    print("Location  "+ str(i + 1)+" :  " + str(popular[i]))


# Services Indexes
Global = []


for timeslot in range(timeslots):
    # Initialize a list to store the sum of bandwidths for each service across all locations
    total_sum = [0] * services

    for location_results in results:
        sum_vals = location_results[timeslot]

        total_sum = [total + sum_val for total, sum_val in zip(total_sum, sum_vals)]

    # Calculate the average for each service
    avg_sum = [total / locations for total in total_sum]

    Global.append(avg_sum)

print("\n \n Average Request Bandwidth \n \n")
for i in range(len(Global)):
    print("timeslot  "+str(i)+": "+str(Global[i]))


desc_order_indexes = []

for avg_sum_row in Global:
    # Get the indexes in descending order based on the elements
    sorted_indexes = sorted(range(len(avg_sum_row)), key=lambda i: avg_sum_row[i], reverse=True)
    desc_order_indexes.append(sorted_indexes)

print("\n \n Global popularity order of services \n \n")
for i in range(len(desc_order_indexes)):
    print(desc_order_indexes[i])


Global_popular = [indexes[:3] for indexes in desc_order_indexes]


print("\n \n Global popular services \n \n")
for i in range(len(Global_popular)):
    print("timeslot  "+str(i)+": "+str(Global_popular[i]))



for i in range(len(popular)):
    for j in range(len(popular[i])):
        popular[i][j] = [index for index in popular[i][j] if index not in desc_order_indexes[j][:3]]

print("\n \n other popular services \n \n")
for i in range(len(popular)):
    print("Location  "+ str(i + 1)+" :  " + str(popular[i]))



print("\n \n locally popular services \n \n")

Local_popular = [[arr[0:2] for arr in row] for row in popular]

for i in range(len(Local_popular)):
    print("Location  " + str(i + 1) + " :  " + str(Local_popular[i]))

#formating the data for Actual values observed

# Define a function to read CSV files and extract values from the second column
def read_csv(file_name):
    values = []
    with open(file_name, 'r', newline='', encoding='utf-8') as file:
        reader = csv.reader(file)
        next(reader)  # Skip the header row if it exists
        for row in reader:
          if row[1] != "":
            values.append(int(float(row[1])))  # Append the value from the second column
    return values

# List of file names
file_names = ['Molotov_Actual.csv', 'Orange_Actual.csv', 'Twitter_Actual.csv', 'Web Games_Actual.csv', 'YouTube_Actual.csv', 'EA_Games_Actual.csv', 'Linkedin_Actual.csv']

# Dictionary to store arrays for each file
data =  {}

# Read each file and store values in the dictionary
for file_name in file_names:
    data[file_name[:-4]] = read_csv(file_name)

# Example usage: Accessing the array of values for each file
for file_name, values in data.items():
    print("Values for", file_name, ":", values)

# Dictionary to store arrays for each position
position_data = {}

# Determine the maximum length of arrays
max_length = max(len(values) for values in data.values())

# Iterate through each position
for i in range(max_length):
    position_array = []
    for file_name, values in data.items():
        if i < len(values):
            position_array.append(values[i])
        else:
            position_array.append(None)  # If the array is shorter, add None as a placeholder
    position_data["Position " + str(i + 1)] = position_array

# Example usage: Accessing the arrays for each position
for position, values in position_data.items():
    print(position + ":", values)

# Function to chunk data into smaller lists of size n
def chunk_list(lst, n):
    return [lst[i:i+n] for i in range(0, len(lst), n)]

# Rearrange data into the desired format
new_data = []
for i in range(max_length):
    new_row = []
    for file_name, values in data.items():
        if i < len(values):
            # print(values[i])
            new_row.append(values[i])
        else:
            new_row.append(None)  # If the array is shorter, add None as a placeholder
        # print(new_row)
    new_data.append(new_row)


# Chunk the new_data into smaller lists of size 960
chunked_data = chunk_list(new_data, 960)

# Prepare column names
columns = ['T' + str(i + 1) for i in range(len(chunked_data[0]))]

# Prepare row names
rows = ['L' + str(i + 1) for i in range(len(chunked_data))]

# Write the chunked_data to a new CSV file
with open('actual_data.csv', 'w', newline='', encoding='utf-8') as file:
    writer = csv.writer(file)

    # Write column names
    writer.writerow([''] + columns)

    # Write row names and data
    for row_name, row_data in zip(rows, chunked_data):
        writer.writerow([row_name] + row_data)

# Load actual_data.csv and predicted_data.csv
actual_data = pd.read_csv("actual_data.csv")
predicted_data = pd.read_csv("predicted_data.csv")

# Copy the first 5 rows of actual_data to formatted_actual_data.csv
formatted_actual_data = actual_data.iloc[:5]
formatted_actual_data.to_csv("formatted_actual_data.csv", index=False)

# Copy the first 5 rows of predicted_data to formatted_predicted_data.csv
formatted_predicted_data = predicted_data.iloc[:5]
formatted_predicted_data.to_csv("formatted_predicted_data.csv", index=False)

# Load actual_data.csv
actual_data = pd.read_csv("actual_data.csv")

# Extract the values from the 6th row without the row index
arrays_to_add = actual_data.iloc[5, 0:961].values.tolist()
arrays_to_add = arrays_to_add[1:]

# Load formatted_actual_data.csv
formatted_actual_data = pd.read_csv("formatted_actual_data.csv")

# Define the number of arrays to extract and the number of columns to add
num_columns_to_add = 192

# Extract and add arrays in a loop
for i in range(1, num_columns_to_add + 1):
    # Extract the arrays from the 6th row
    start_index = (i - 1) * 5
    end_index = i * 5

    # Add the extracted arrays as a new column
    column_name = f"T{960 + i}"
    formatted_actual_data[column_name] = arrays_to_add[start_index:end_index]

# Save the modified DataFrame to formatted_actual_data.csv
formatted_actual_data.to_csv("formatted_actual_data.csv", index=False)

# Load actual_data.csv
actual_data = pd.read_csv("actual_data.csv")

# Extract the values from the 6th row without the row index
arrays_to_add = actual_data.iloc[5, 0:961].values.tolist()
arrays_to_add = arrays_to_add[1:]

# Load formatted_actual_data.csv
formatted_actual_data = pd.read_csv("formatted_actual_data.csv")

# Define the number of arrays to extract and the number of columns to add
num_columns_to_add = 192

# Extract and add arrays in a loop
for i in range(1, num_columns_to_add + 1):
    # Extract the arrays from the 6th row
    start_index = (i - 1) * 5
    end_index = i * 5

    # Add the extracted arrays as a new column
    column_name = f"T{1152 + i}"
    formatted_actual_data[column_name] = arrays_to_add[start_index:end_index]

# Save the modified DataFrame to formatted_actual_data.csv
formatted_actual_data.to_csv("formatted_actual_data.csv", index=False)

# Load actual_data.csv
actual_data = pd.read_csv("actual_data.csv")

# Extract the values from the 6th row without the row index
arrays_to_add = actual_data.iloc[7, 0:601].values.tolist()
arrays_to_add = arrays_to_add[1:]

# Load formatted_actual_data.csv
formatted_actual_data = pd.read_csv("formatted_actual_data.csv")

# Define the number of arrays to extract and the number of columns to add
num_columns_to_add = 120

# Extract and add arrays in a loop
for i in range(1, num_columns_to_add + 1):
    # Extract the arrays from the 6th row
    start_index = (i - 1) * 5
    end_index = i * 5

    # Add the extracted arrays as a new column
    column_name = f"T{1344 + i}"
    formatted_actual_data[column_name] = arrays_to_add[start_index:end_index]

# Save the modified DataFrame to formatted_actual_data.csv
formatted_actual_data.to_csv("formatted_actual_data.csv", index=False)

# Load predicted_data.csv
predicted_data = pd.read_csv("predicted_data.csv")

# Extract the values from the 6th row without the row index
arrays_to_add = predicted_data.iloc[5, 0:961].values.tolist()
arrays_to_add = arrays_to_add[1:]

# Load formatted_predicted_data.csv
formatted_predicted_data = pd.read_csv("formatted_predicted_data.csv")

# Define the number of arrays to extract and the number of columns to add
num_columns_to_add = 192

# Extract and add arrays in a loop
for i in range(1, num_columns_to_add + 1):
    # Extract the arrays from the 6th row
    start_index = (i - 1) * 5
    end_index = i * 5

    # Add the extracted arrays as a new column
    column_name = f"T{960 + i}"
    formatted_predicted_data[column_name] = arrays_to_add[start_index:end_index]

# Save the modified DataFrame to formatted_predicted_data.csv
formatted_predicted_data.to_csv("formatted_predicted_data.csv", index=False)

# Load predicted_data.csv
predicted_data = pd.read_csv("predicted_data.csv")

# Extract the values from the 6th row without the row index
arrays_to_add = predicted_data.iloc[5, 0:961].values.tolist()
arrays_to_add = arrays_to_add[1:]

# Load formatted_predicted_data.csv
formatted_predicted_data = pd.read_csv("formatted_predicted_data.csv")

# Define the number of arrays to extract and the number of columns to add
num_columns_to_add = 192

# Extract and add arrays in a loop
for i in range(1, num_columns_to_add + 1):
    # Extract the arrays from the 6th row
    start_index = (i - 1) * 5
    end_index = i * 5

    # Add the extracted arrays as a new column
    column_name = f"T{1152 + i}"
    formatted_predicted_data[column_name] = arrays_to_add[start_index:end_index]

# Save the modified DataFrame to formatted_predicted_data.csv
formatted_predicted_data.to_csv("formatted_predicted_data.csv", index=False)

# Load predicted_data.csv
predicted_data = pd.read_csv("predicted_data.csv")

# Extract the values from the 6th row without the row index
arrays_to_add = predicted_data.iloc[7, 0:601].values.tolist()
arrays_to_add = arrays_to_add[1:]

# Load formatted_predicted_data.csv
formatted_predicted_data = pd.read_csv("formatted_predicted_data.csv")

# Define the number of arrays to extract and the number of columns to add
num_columns_to_add = 120

# Extract and add arrays in a loop
for i in range(1, num_columns_to_add + 1):
    # Extract the arrays from the 6th row
    start_index = (i - 1) * 5
    end_index = i * 5

    # Add the extracted arrays as a new column
    column_name = f"T{1344 + i}"
    formatted_predicted_data[column_name] = arrays_to_add[start_index:end_index]

# Save the modified DataFrame to formatted_predicted_data.csv
formatted_predicted_data.to_csv("formatted_predicted_data.csv", index=False)

# Assigning Bandwidth for the slots and setting up reward by comparing them to the actual usage

# Bandwidth range allocations using number of requests
LB_Number_of_Requests = 300
MB_Number_of_Requests = 2000
HB_Number_of_Requests = 5000

# Penality Values declaration
LB_Penality_value = 200
MB_Penality_value = 1500
HB_Penality_value = 4000

# Bandwidth range allocations everything is in Mbps
LB_range = 50
MB_range = 600
HB_range = 1000



for i in range(1, 3):
    column_values = formatted_predicted_data['T' + str(960 + i)]
    Actual_timeslot_value = formatted_actual_data['T' + str(960 + i)]
    # print("T" + str(960 + i) + "----------" + str(960 + i), end=" ")

    if ((960 + i) % 96) -1 < 0:
      index = 95
    else:
      index =  ((960 + i) % 96) -1
    # print(index, end=" ")


    bandwidth_array = [['L','L','L','L','L','L','L'],['L','L','L','L','L','L','L'],['L','L','L','L','L','L','L'],['L','L','L','L','L','L','L'],['L','L','L','L','L','L','L']]

    #  Global Popular service bandwidth allocation
    for service in range(7):
        if service in Global_popular[index]:
            for location_each in range(5):
                bandwidth_array[location_each][service] = 'H'
    # print(Global_popular[index],end=" ")
    # print(bandwidth_array)


    #  Local Popular service bandwidth allocation
    for location_each in range(5):
        for service in range(7):
            if service in Local_popular[index][location_each]:
                list_of_integers_for_predictted = ast.literal_eval(column_values[location_each])
                if list_of_integers_for_predictted[service] >= 2000:
                  bandwidth_array[location_each][service] = 'H'
                else:
                  bandwidth_array[location_each][service] = 'M'
                # print("---"+str(list_of_integers_for_predictted[service])+"---")
        # print(Local_popular[index][location_each],Global_popular[index], end=" ")
        # print(bandwidth_array[location_each])


    for location_each in range(5):
      for service in range(7):
        list_of_integers_for_actual = ast.literal_eval(column_values[location_each])
        percentage = 0
        Bandwidth_actual_usage = 0
        Allotted_bandwidth_symbol = 'L'
        Allotted_bandwidth = LB_range

        # if (list_of_integers_for_actual[service] == 0 )
        if( list_of_integers_for_actual[service] <= LB_Number_of_Requests and list_of_integers_for_actual[service] > 0) :
          percentage = list_of_integers_for_actual[service] / LB_Number_of_Requests
          Bandwidth_actual_usage = percentage * LB_range
        elif ( list_of_integers_for_actual[service] <= MB_Number_of_Requests ):
          percentage = list_of_integers_for_actual[service] / MB_Number_of_Requests
          Bandwidth_actual_usage = percentage * MB_range
        else:
          percentage = list_of_integers_for_actual[service] / HB_Number_of_Requests
          Bandwidth_actual_usage = percentage * HB_range

        # print the bandwidth that actaully being used and the slice that we made
        print( Bandwidth_actual_usage, bandwidth_array[location_each][service],end=" ")

        Allotted_bandwidth_symbol = bandwidth_array[location_each][service]

        if Allotted_bandwidth_symbol == 'H':
          Allotted_bandwidth = HB_range
        elif Allotted_bandwidth_symbol == 'M':
          Allotted_bandwidth = MB_range
        else:
          Allotted_bandwidth = LB_range


        Reward_Computed = 0

        if( Bandwidth_actual_usage <  Allotted_bandwidth ) :
            Reward_Computed = 1- ((Allotted_bandwidth - Bandwidth_actual_usage) / Allotted_bandwidth)
        else :
            Reward_Computed = 1- ((Bandwidth_actual_usage - Allotted_bandwidth) / Bandwidth_actual_usage)

        print(Reward_Computed)



        # print(list_of_integers_for_actual[service],bandwidth_array[location_each][service])
    #     if list_of_integers_for_actual[service] <= LB_Number_of_Requests:
    #       percentage = LB_Number_of_Requests / ((list_of_integers_for_actual[service] > 0 )? list_of_integers_for_actual[service] : LB_Number_of_Requests )

